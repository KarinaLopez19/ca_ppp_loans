{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#setting pandas display options\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "pd.set_option('display.max_colwidth', 199)  # or 199\n",
    "\n",
    "BASE_DIR = \"/home/jovyan/work/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times-provided per-county COVID case loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NYTCCL has a few points of missing data. \n",
    "\n",
    "### Reporting start time and frequency\n",
    "\n",
    "1. Different counties started tracking their COVID caseloads at different dates, so they are missing data before they started measuring and reporting\n",
    "2. Different counties report at different intervals. Whereas some counties reported daily counts, others reported only a few times a week\n",
    "3. Some counties had irregular gaps up to a few days between measurements\n",
    "\n",
    "#### Response\n",
    "\n",
    "For the first case, we can make either disregard those dates, or assume that their case loads were zero. In our analyses, we will likely be interested only in the cases at later dates during the pandemic or in growth rates. It is unlikely that our choice to disregard these dates or mark them as zero will have a large impact on our analysis. \n",
    "\n",
    "For the second and third scenarios, we can impute values via either\n",
    "\n",
    "- Linear interpolation, or\n",
    "- Assuming no change from the previous measurement\n",
    "\n",
    "Judging from the plots shown below, the plots appear rather smooth, so there isn't much to be gained by interpolating values. We will assume they have not changed from the previous measurement. We can perform our analysis with both the linearly interpolated and left-alone values to compare in the end. \n",
    "\n",
    "### California cases without county name and FIPS number\n",
    "\n",
    "The second category of missing data here is that there are a handful (a dozen or so) cases that are not associated with any county in California. These dozen or so cases are a only a miniscule fraction of the nearly million cases in California, so we will be disregarding them and simply dropping those rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_csv_parameters = {\n",
    "    'parse_dates': ['date'],\n",
    "    'dtype': {\n",
    "        'county': 'string',\n",
    "        'state': 'string',\n",
    "        'fips': 'UInt64',\n",
    "        'cases': 'UInt64',\n",
    "        'deaths': 'UInt64'\n",
    "    }\n",
    "}\n",
    "\n",
    "nytccl = pandas.read_csv(BASE_DIR + 'data-sets/raw-datasets/us-counties.csv', **read_csv_parameters)\n",
    "nytccl = nytccl[ nytccl['state'] == 'California' ] \n",
    "\n",
    "nytccl.drop(nytccl[ nytccl['fips'].isnull() ].index, inplace=True)\n",
    "\n",
    "nytccl = nytccl.reset_index() \\\n",
    "    .drop(columns=['index'])\n",
    "\n",
    "lastDateInDataset = nytccl.tail(1)['date'].iloc[0]\n",
    "\n",
    "print(nytccl[ nytccl['date'] == lastDateInDataset ].sum())\n",
    "\n",
    "for title, group in nytccl.groupby(['county']):\n",
    "    group.plot(x = 'date', y = 'cases', title = title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nytccl.tail(1)['date'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury department PPP loans dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loans under $150,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edits by: Karina Lopez\n",
    "\n",
    "Input fname: ca_PPP_150kunder.csv\n",
    "<br>Output fname: ca_ppp_loans_under_150k.csv\n",
    "\n",
    "<br>*The PPP loan dataset is divided into two separate datasets. This dataset only contains PPP loans up to 150K.*\n",
    "\n",
    "**Datacleaning process:**\n",
    "- Column names were changed based on the standards we set as a team\n",
    "- Proportion of missing data was checked for each column. Columns with over 80% of data missing were identified and added to read me file\n",
    "- Strings were fixed to remove unnecessary characters\n",
    "- Data was converted to approriate data types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + 'data-sets/raw-datasets/120120 Paycheck Protection Program Data/')\n",
    "\n",
    "# Load in your datasets\n",
    "PPP_df = pd.read_csv('01 PPP sub 150k through 112420.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only include CA data\n",
    "\n",
    "PPP_df = PPP_df[PPP_df['State'] == 'CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace unspecified data w/ NAs\n",
    "PPP_df['RaceEthnicity'] = PPP_df['RaceEthnicity'].replace('Unanswered', np.NaN)\n",
    "PPP_df['Gender'] = PPP_df['Gender'].replace('Unanswered', np.NaN)\n",
    "PPP_df['Veteran'] = PPP_df['Veteran'].replace('Unanswered', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PPP_df.isnull().sum() * 100 / len(PPP_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PPP_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since zipcode is really important for pairing to our county data, check if you can fill in the missing zipcode w/ city name\n",
    "PPP_df_NA = PPP_df[PPP_df.Zip.isnull()]\n",
    "display(PPP_df_NA)\n",
    "\n",
    "# Since we do not have business names or any city names, we will keep the missing data in their own category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to lowercase strings\n",
    "PPP_df = PPP_df.apply(lambda x: x.astype(str).str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPP_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns so thet can be matched to other datasets\n",
    "\n",
    "PPP_df.rename(columns = {'LoanAmount':'loan_amount', 'City':'city', 'Zip':'zipcode', 'NAICSCode':'NAICS_code', 'BusinessType':'business_type',\n",
    "                         'JobsReported':'jobs_reported', 'DateApproved':'date_approved', 'Lender':'lender', 'State': 'state',\n",
    "                         'RaceEthnicity':'race_ethnicity', 'Gender':'gender', 'Veteran':'veteran', 'NonProfit':'non_profit'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPP_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPP_df.loan_amount = pd.to_numeric(PPP_df.loan_amount, errors = 'coerce')\n",
    "PPP_df.zipcode = pd.to_numeric(PPP_df.zipcode, errors = 'coerce')\n",
    "PPP_df.NAICS_code = pd.to_numeric(PPP_df.NAICS_code, errors = 'coerce')\n",
    "PPP_df.jobs_reported = pd.to_numeric(PPP_df.jobs_reported, errors = 'coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long workaround to put in nan for missing data\n",
    "PPP_df.loan_amount = PPP_df.loan_amount.fillna(-1)\n",
    "PPP_df.loan_amount = PPP_df.loan_amount.astype(int)\n",
    "PPP_df.loan_amount = PPP_df.loan_amount.astype(str)\n",
    "PPP_df.loan_amount = PPP_df.loan_amount.replace('-1', np.nan)\n",
    "\n",
    "PPP_df.zipcode = PPP_df.zipcode.fillna(-1)\n",
    "PPP_df.zipcode = PPP_df.zipcode.astype(int)\n",
    "PPP_df.zipcode = PPP_df.zipcode.astype(str)\n",
    "PPP_df.zipcode = PPP_df.zipcode.replace('-1', np.nan)\n",
    "\n",
    "PPP_df.NAICS_code = PPP_df.NAICS_code.fillna(-1)\n",
    "PPP_df.NAICS_code = PPP_df.NAICS_code.astype(int)\n",
    "PPP_df.NAICS_code = PPP_df.NAICS_code.astype(str)\n",
    "PPP_df.NAICS_code = PPP_df.NAICS_code.replace('-1', np.nan)\n",
    "\n",
    "PPP_df.jobs_reported = PPP_df.jobs_reported.fillna(-1)\n",
    "PPP_df.jobs_reported = PPP_df.jobs_reported.astype(int)\n",
    "PPP_df.jobs_reported = PPP_df.jobs_reported.astype(str)\n",
    "PPP_df.jobs_reported = PPP_df.jobs_reported.replace('-1', np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PPP_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PPP_df.shape)\n",
    "PPP_df.head(n = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPP_df.business_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPP_df.columns = PPP_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a new \"cleaned up\" csv\n",
    "os.chdir(BASE_DIR + 'data-sets/clean-datasets/')\n",
    "PPP_df.to_csv('ca_ppp_loans_under_150k.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loans over $150,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains PPP loans over 150k.\n",
    "\n",
    "Summary of Data-cleaning process:\n",
    "\n",
    "- Change column names according to naming convention established by team (lowercase, all spaces underscored)\n",
    "- Find rows where State column is null, then check CD column for any rows that are CA businesses\n",
    "- Filter rows where State is CA, add the rows from the previous step, then use this new dataframe since we are only looking at CA businesses\n",
    "- Change all strings in dataset to be lowercase\n",
    "- Change column values to appropriate datatypes\n",
    "- Check percent of missing values for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ppp = pd.read_csv(BASE_DIR + 'data-sets/raw-datasets/120120 Paycheck Protection Program Data/150k plus PPP through 112420.csv')\n",
    "new_ppp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the column names to lowercase and add underscores\n",
    "new_ppp.columns = new_ppp.columns.str.lower()\n",
    "new_ppp = new_ppp.rename(columns={\"loanamount\": \"loan_amount\", \"businessname\": \"business_name\", \n",
    "                         \"naicscode\":\"naics_code\", \"businesstype\":\"business_type\",\n",
    "                        \"raceethnicity\":\"race_ethnicity\", \"nonprofit\":\"non_profit\",\n",
    "                         \"jobsreported\":\"jobs_reported\", \"dateapproved\":\"date_approved\", \"zip\":\"zipcode\"})\n",
    "new_ppp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all rows where state is null\n",
    "new_ppp[new_ppp['state'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the rows with no given state, in the cd column we see that one row definitely belongs to CA\n",
    "#save this row to add later \n",
    "new_ca_row = new_ppp[new_ppp['business_name']=='KIRTLEY CONSTRUCTION INC']\n",
    "\n",
    "#filter dataset to to only include CA businesses\n",
    "#then append the row above to dataset\n",
    "new_ppp = new_ppp[new_ppp['state'] == 'CA']\n",
    "new_ppp_150_plus = pd.concat([new_ppp, new_ca_row]).reset_index(drop=True)\n",
    "new_ppp_150_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the appended row still has NaN as its state value, fill it in with CA\n",
    "new_ppp_150_plus['state'] = new_ppp_150_plus['state'].fillna('CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all strings in dataset to be lowercase\n",
    "new_ppp_150_plus = new_ppp_150_plus.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date_approved column to be in datetime format\n",
    "new_ppp_150_plus['date_approved'] = pd.to_datetime(new_ppp_150_plus['date_approved'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change loan amount type to int\n",
    "new_ppp_150_plus['loan_amount'] = new_ppp_150_plus['loan_amount'].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all 'nan' values with NaN so it is a python-recognized missing value\n",
    "new_ppp_150_plus = new_ppp_150_plus.replace('nan', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change zipcode, naics_code and jobs_reported columns types to integer without removing NA values\n",
    "new_ppp_150_plus['zipcode'] = pd.to_numeric(new_ppp_150_plus['zipcode'], errors='coerce').astype('Int64')\n",
    "new_ppp_150_plus['naics_code'] = pd.to_numeric(new_ppp_150_plus['naics_code'], errors='coerce').astype('Int64')\n",
    "new_ppp_150_plus['jobs_reported'] = pd.to_numeric(new_ppp_150_plus['jobs_reported'], errors='coerce').astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print % of null values for each column at this point\n",
    "print(new_ppp_150_plus.isnull().sum() * 100 / len(new_ppp_150_plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for columns that have 'unanswered' as a column value, replace with NaN to see how this changes null %\n",
    "new_ppp_150_plus['race_ethnicity'] = new_ppp_150_plus['race_ethnicity'].replace('unanswered', np.NaN)\n",
    "new_ppp_150_plus['gender'] = new_ppp_150_plus['gender'].replace('unanswered', np.NaN)\n",
    "new_ppp_150_plus['veteran'] = new_ppp_150_plus['veteran'].replace('unanswered', np.NaN)\n",
    "print(new_ppp_150_plus.isnull().sum() * 100 / len(new_ppp_150_plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ppp_150_plus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ppp_150_plus.to_csv(BASE_DIR + 'data-sets/clean-datasets/ca_new_ppp_loans_150_plus_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California zipcodes and counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edits by: Karina Lopez\n",
    "\n",
    "Input fname: ca_census_county_zipcodes.csv\n",
    "<br>Output fname: ca_zipcodes.csv\n",
    "\n",
    "<br>*The zipcode dataset contains all zipcodes and cities in California. This data set can be used as a merging key*\n",
    "\n",
    "**Datacleaning process:**\n",
    "- Completely empty rows (no data in any columns) were removed from the dataset\n",
    "- County names that were missing were filled in by searching for the county the city entry belomged to in Google\n",
    "- Data types were converted to the appropriate typing (e.g., zipcodes are integers)\n",
    "- Strings were all converted to lowercase\n",
    "- Column names were all converted to lowercase to abide by dataset standards set by the team (eases merging process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + 'data-sets/raw-datasets/')\n",
    "\n",
    "# Load in your datasets\n",
    "zipcodes_df = pd.read_csv('ca_census_county_zipcodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(zipcodes_df.isnull().sum())\n",
    "\n",
    "# Show rows w/ missing values\n",
    "zipcodes_df_NA = zipcodes_df[zipcodes_df.isnull().any(axis=1)]\n",
    "display(zipcodes_df_NA )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually looked up counties () and updated their values\n",
    "# La Quinta: https://en.wikipedia.org/wiki/La_Quinta,_California (riverside)\n",
    "# Oceano: https://en.wikipedia.org/wiki/Oceano,_California (san luis obispo)\n",
    "\n",
    "zipcodes_df.at[746, 'county'] = 'Riverside'\n",
    "zipcodes_df.at[747, 'county'] = 'Riverside'\n",
    "zipcodes_df.at[1278, 'county'] = 'San Luis Obispo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all NA values remaining\n",
    "zipcodes_df = zipcodes_df.dropna() \n",
    "print(zipcodes_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "zipcodes_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to lowercase\n",
    "zipcodes_df = zipcodes_df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes_df.zipcode = pd.to_numeric(zipcodes_df.zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert zipcodes to integers\n",
    "zipcodes_df.zipcode = zipcodes_df.zipcode.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names for city in dataset\n",
    "zipcodes_df.rename(columns = {'zipcode_name':'city'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zipcodes_df.dtypes)\n",
    "print(zipcodes_df.shape)\n",
    "zipcodes_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes_df.zipcode_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a new \"cleaned up\" csv\n",
    "os.chdir(BASE_DIR + 'data-sets/clean-datasets/')\n",
    "zipcodes_df.to_csv('ca_zipcodes.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census urban and rural populations by county"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edits by: Karina Lopez\n",
    "\n",
    "Input fname: ca_census_rural.csv\n",
    "<br>Output fname: ca_county_population.csv\n",
    "\n",
    "<br>*The census population dataset contains population counts for each county. It idenitifies population counts and percentages for both rural and urban areas in each county.*\n",
    "\n",
    "**Datacleaning process:**\n",
    "- Column names were changed based on the standards we set as a team\n",
    "- Strings were fixed to remove unnecessary characters\n",
    "- Data was converted to approriate data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(BASE_DIR + 'data-sets/raw-datasets/')\n",
    "\n",
    "# Load in your datasets\n",
    "census_df = pd.read_csv('ca_census_rural.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "print(census_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names in dataset\n",
    "census_df.rename(columns = {'2015 GEOID': 'GEOID', 'State': 'state', '2010 Census Urban Population':'urban_population', '2015 Geography Name':'county', '2010 Census Total Population':'total_population',\n",
    "                             '2010 Census Rural Population':'rural_population', '2010 Census Percent Rural':'rural_percent'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove strings from each county column\n",
    "census_df['county'] = census_df['county'].str.rstrip(', California')\n",
    "census_df['county'] = census_df['county'].str.rstrip('County')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to lowercase\n",
    "census_df = census_df.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "census_df['county'] = census_df['county'].apply(lambda x:x.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(census_df.dtypes)\n",
    "\n",
    "# some numerical columns are strings (object). We need to remove commas to convert them to number columns\n",
    "census_df.replace(',','', regex = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert number data to integers and floats\n",
    "census_df.total_population = pd.to_numeric(census_df.total_population)\n",
    "census_df.urban_population = pd.to_numeric(census_df.urban_population)\n",
    "census_df.rural_population = pd.to_numeric(census_df.rural_population)\n",
    "census_df.rural_percent = pd.to_numeric(census_df.rural_percent)\n",
    "census_df.GEOID = pd.to_numeric(census_df.GEOID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(census_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(census_df.shape)\n",
    "census_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df.columns = census_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a new \"cleaned up\" csv\n",
    "os.chdir(BASE_DIR + 'data-sets/clean-datasets/')\n",
    "census_df.to_csv('ca_county_population.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry classification codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a set of codes and titles used to uniquely identify each industry in the United States.\n",
    "\n",
    "Summary of Data-cleaning process:\n",
    "\n",
    "- Drop column with all missing values\n",
    "- Remove row with all NaN values\n",
    "- Change column names according to naming convention established by team (lowercase, all spaces underscored)\n",
    "- Lowercase title column\n",
    "- Change NAICS code column type to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import industry codes csv \n",
    "industry_codes = pd.read_csv(BASE_DIR + 'data-sets/raw-datasets/industry_codes_raw.csv')\n",
    "industry_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see if there are any values that aren't missing in the 'Unnamed: 2' column\n",
    "industry_codes[industry_codes['Unnamed: 2'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of missing values in 'Unnamed: 2' column out of 1058 total rows\n",
    "industry_codes['Unnamed: 2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip the first row since it's all NaN values\n",
    "#drop the 'Unnamed: 2' since it has all NaN values\n",
    "industry_codes = industry_codes[1:].reset_index(drop=True)\n",
    "industry_codes = industry_codes.drop('Unnamed: 2', axis = 1) \n",
    "\n",
    "industry_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names to lowercase and add underscores\n",
    "#change'2017_naics_title' to be all lowercase\n",
    "#change'2017_naics_code' type to integer\n",
    "industry_codes.columns = ['2017_naics_code', '2017_naics_title']\n",
    "industry_codes['2017_naics_title'] = industry_codes['2017_naics_title'].str.lower()\n",
    "industry_codes['2017_naics_code'] = industry_codes['2017_naics_code'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_codes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_codes.to_csv(BASE_DIR + 'data-sets/clean-datasets/industry_codes_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment by major industry sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset shows the number of jobs per major industry in 2009, 2019, and an estimate for 2029, in thousands.\n",
    "\n",
    "Summary of Data-cleaning process:\n",
    "\n",
    "- Drop rows and columns that contained all NaN values\n",
    "- Convert first two rows of dataset into column names (were misplaced due to how they were imported)\n",
    "- Change column names according to naming convention established by team (lowercase, all spaces underscored)\n",
    "- Remove commas in numbers, change number types to float since they include important decimals\n",
    "- Remove unnecessary parenthesis and numbers in the industry title column, make column lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import industry csv\n",
    "industry = pd.read_csv(BASE_DIR + 'data-sets/raw-datasets/industry_job_counts_raw.csv')\n",
    "\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep rows that aren't all NaN\n",
    "industry = industry.copy()[:31]\n",
    "industry = industry.drop([3, 5, 10, 25, 29]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop last four columns that contain all NaN values\n",
    "industry = industry.drop(industry.iloc[:, 11:15], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make first row the column names for more clarity, then delete that row since it has no information\n",
    "industry.columns = industry.iloc[0]\n",
    "industry = industry[1:].reset_index(drop=True)\n",
    "\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column names using an understandable naming convention\n",
    "industry.columns = ['industry_title','thousands_of_jobs_2009', 'thousands_of_jobs_2019', 'thousands_of_jobs_2029', 'change_2009_2019', 'change_2019_2029', 'pct_distribution_2009','pct_distribution_2019', 'pct_distribution_2029', 'cmpd_annual_rate_of_change_2009_2019', 'cmpd_annual_rate_of_change_2009_2019']\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the first row since it is now incorporated in the column names\n",
    "industry = industry.copy()[1:]\n",
    "\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the commas for all column values that contain them\n",
    "#keep as float since we are looking at thousands of jobs, so decimals might be important\n",
    "for col in industry.columns[1:6]:\n",
    "    industry[col]=industry[col].str.replace(',', '').astype(float)\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry['industry_title'] = industry['industry_title'].str.replace('[(0-9)]', '', regex=True).str.lower()\n",
    "industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry.to_csv(BASE_DIR + 'data-sets/clean-datasets/industry_job_counts_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census population counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labor Force and Unemployment Rate for California Counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Labor Force & Unemployment Rate for California Counties dataset provides totals for labor force, employed, and unemployed workers in California counties month to month, calculating an unemployment rate based on those numbers. \n",
    "\n",
    "The datacleaning process for this dataset wasn't too difficult. The dataset did not have any null values, only providing information on areas and years in which it was available. Once the dataset was formatted based on team parameters/standards, it was then adjusted in the following ways:\n",
    "\n",
    "- The original dataset provided numbers not just on counties, but on other types of areas as well (large cities, municipal areas, the state as a whole). We isolated the data to only county-specific numbers.\n",
    "- We dropped the 'status' column, which confirms whether the data is final or preliminary. Preliminary data only is relevant for the most recent month entry, which we believe will not have an effect on our analysis of the data.\n",
    "- All counties, with the exception of Los Angeles County, had data that was not seasonally adjusted. Los Angeles provided both seasonally and non-seasonally adjusted data. To stay consistent, we removed the seasonally adjusted Los Angeles County entry, and subsequently removed the Seasonally Adjusted column. \n",
    "- Given that we are interested in the impact of the PPP Loans, which occurred over this past year, we limited the size of the dataset to entries from 2020 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR + 'data-sets/raw-datasets/ca_employment_raw_sept2020.csv', sep=',')\n",
    "counties_df = df.copy()\n",
    "counties_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is to format the df to team-specified standards\n",
    "counties_df.columns = counties_df.columns.str.lower()\n",
    "counties_df.rename(columns = lambda x: x.strip(), inplace=True)\n",
    "counties_df.columns = counties_df.columns.str.replace(' ', '_')\n",
    "counties_df.rename(columns={'seasonally_adjusted_(y/n)':'seasonally_adjusted', 'status_(preliminary_/_final)':'status'}, inplace=True)\n",
    "counties_df['date'] = pd.to_datetime(counties_df['date'])\n",
    "cols_to_change = ['area_type', 'month', 'seasonally_adjusted', 'status']\n",
    "\n",
    "for col in cols_to_change:\n",
    "    counties_df[col] = counties_df[col].str.lower().str.replace(' ',  '_')\n",
    "    \n",
    "counties_df['area_name'] = counties_df['area_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolating county data only\n",
    "counties_df = counties_df[counties_df['area_type'] == 'county']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns\n",
    "counties_df.drop(columns=['area_type', 'status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolating 2020 data only\n",
    "counties_2020_df = counties_df[counties_df['year'] == 2020]\n",
    "counties_2020_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing LA County Seasonally adjusted data, seasonally adjusted column\n",
    "counties_2020_df = counties_2020_df[counties_2020_df.seasonally_adjusted != 'y']\n",
    "counties_2020_df.drop(columns=['seasonally_adjusted'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_2020_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## United States Temporary Business Closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in this set are entirely complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "outputs": [],
   "source": [
    "api = pd.read_csv('/home/jovyan/work/team-work/data-sets/raw-datasets/US_temporary_closures_by_State.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's go through the columns present in the dataframe:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have the following columns</p>\n",
    "<ol>\n",
    "<li><strong>date</strong>: The date in which the information was retreived</li>\n",
    "<li><strong>index</strong>: The degree of how closed the businesses were on that date. Permanently closed > 100</li>\n",
    "<li><strong>close_reason</strong>: Closed or temporarily</li>\n",
    "<li><strong>region</strong>: State</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that `date` is a date, and that `close_reason` and `region` are strings, I will make sure to properly read these with `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = pd.read_csv('/home/jovyan/work/team-work/data-sets/raw-datasets/US_temporary_closures_by_State.csv', \n",
    "\tparse_dates=['date'], \n",
    "\tdtype={\n",
    "\t\t'close_reason': 'string',\n",
    "\t\t'region': 'string'\n",
    "\t}\n",
    ")\n",
    "api.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing I need to do is extract just California information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_ca = api.loc[api.region == 'California']\n",
    "api_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_ca.groupby(api_ca.date.dt.month).index.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Taxable Sales by County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are mainly using this dataset in order to draw California's counties. For some reason, however, the dataset repeats the shapes of the counties over and over again for every single year. To cut down on the size of this dataset, we have extracted only the 2019 data and saved the shapes from there. This reduced the size of the geoJSON file from ~307MiB to only ~16MiB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import PythonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "def show_function(function):\n",
    "\n",
    "\tprint(highlight(inspect.getsource(function), lexer, formatter))\n",
    "\n",
    "def shoddy_import(path):\n",
    "\n",
    "    _temp = __import__('.'.join(path))\n",
    "\n",
    "    for submodule in path[1:]:\n",
    "\n",
    "        _temp = getattr(_temp, submodule)\n",
    "\n",
    "    return _temp\n",
    "\n",
    "os.chdir(BASE_DIR)\n",
    "\n",
    "explore = shoddy_import([ \"scripts\", \"exploratory-analyses\", \"dacoda-project-scope\", \"explore\" ])\n",
    "\n",
    "lexer = PythonLexer()\n",
    "formatter = TerminalFormatter()\n",
    "\n",
    "show_function(explore.trim)\n",
    "show_function(explore.dataFromScratch)\n",
    "show_function(explore.grabData)\n",
    "\n",
    "data = explore.grabData()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
